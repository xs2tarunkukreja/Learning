{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b909dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/ec2-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 175MB/s]\n"
     ]
    }
   ],
   "source": [
    "# we are going to perform single training step - forward and then backward.\n",
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80bfa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fa3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss\n",
    "# backpropagate this error\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63066847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimizer, SGD learning rate=0.01 and momentum = 0.9\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "optim.step() # Initiate gradient descent. optimizer adjusts each parameter by its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8dff79",
   "metadata": {},
   "source": [
    "# Differentiation in Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0a2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True) # True means now all operation will be tracked on this tensor.\n",
    "b = torch.tensor([6., 4.], requires_grad=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75d44b",
   "metadata": {},
   "source": [
    "create another tensor $$Q = 3a^3 - b^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eae7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf4a01",
   "metadata": {},
   "source": [
    "Q is an error and a and b are 2 parameters of NN.\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial b} = -2b$$\n",
    "\n",
    "When we call backward() on Q, autograd calculates these gradients and stores them in the respective tensor's `.grad` attribute.\n",
    "\n",
    "We need to explicitly pass a gradient argument in Q.backward() because it is a vector. gradient is a tensor of the same shape as Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4787c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# Gradients are now deposited in a.grad and b.grad\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe6b81",
   "metadata": {},
   "source": [
    "Optional Reading - Vector Calculus using `autograd`\n",
    "===================================================\n",
    "\n",
    "Mathematically, if you have a vector valued function\n",
    "$\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to\n",
    "$\\vec{x}$ is a Jacobian matrix $J$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J\n",
    "=\n",
    " \\left(\\begin{array}{cc}\n",
    " \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n",
    " ... &\n",
    " \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Generally speaking, `torch.autograd` is an engine for computing\n",
    "vector-Jacobian product. That is, given any vector $\\vec{v}$, compute\n",
    "the product $J^{T}\\cdot \\vec{v}$\n",
    "\n",
    "If $\\vec{v}$ happens to be the gradient of a scalar function\n",
    "$l=g\\left(\\vec{y}\\right)$:\n",
    "\n",
    "$$\\vec{v}\n",
    " =\n",
    " \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$$\n",
    "\n",
    "then by the chain rule, the vector-Jacobian product would be the\n",
    "gradient of $l$ with respect to $\\vec{x}$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J^{T}\\cdot \\vec{v}=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    " \\end{array}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "This characteristic of vector-Jacobian product is what we use in the\n",
    "above example; `external_grad` represents $\\vec{v}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32978982",
   "metadata": {},
   "source": [
    "Computational Graph\n",
    "===================\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) & all executed\n",
    "operations (along with the resulting new tensors) in a directed acyclic\n",
    "graph (DAG) consisting of\n",
    "[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
    "objects. In this DAG, leaves are the input tensors, roots are the output\n",
    "tensors. By tracing this graph from roots to leaves, you can\n",
    "automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "-   run the requested operation to compute a resulting tensor, and\n",
    "-   maintain the operation's *gradient function* in the DAG.\n",
    "\n",
    "The backward pass kicks off when `.backward()` is called on the DAG\n",
    "root. `autograd` then:\n",
    "\n",
    "-   computes the gradients from each `.grad_fn`,\n",
    "-   accumulates them in the respective tensor's `.grad` attribute, and\n",
    "-   using the chain rule, propagates all the way to the leaf tensors.\n",
    "\n",
    "Below is a visual representation of the DAG in our example. In the\n",
    "graph, the arrows are in the direction of the forward pass. The nodes\n",
    "represent the backward functions of each operation in the forward pass.\n",
    "The leaf nodes in blue represent our leaf tensors `a` and `b`.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/dag_autograd.png)\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>An important thing to note is that the graph is recreated from scratch; after each<code>.backward()</code> call, autograd starts populating a new graph. This isexactly what allows you to use control flow statements in your model;you can change the shape, size and operations at every iteration ifneeded.</p>\n",
    "</div>\n",
    "\n",
    "Exclusion from the DAG\n",
    "----------------------\n",
    "\n",
    "`torch.autograd` tracks operations on all tensors which have their\n",
    "`requires_grad` flag set to `True`. For tensors that don't require\n",
    "gradients, setting this attribute to `False` excludes it from the\n",
    "gradient computation DAG.\n",
    "\n",
    "The output tensor of an operation will require gradients even if only a\n",
    "single input tensor has `requires_grad=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20374f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a NN, parameters that don’t compute gradients are usually called frozen parameters.\n",
    "# Let's freeze all parameters.\n",
    "\n",
    "from torch import nn, optim\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fef01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are replacing last linear layer model.fc with new linear layer. So, new layer is unfrozen.\n",
    "model.fc = nn.Linear(512,10) # 10 labels.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
